{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker PySpark XGBoost MNIST\n",
    "\n",
    "1. [Введение](#Введение)\n",
    "2. [Установка](#Установка)\n",
    "3. [Загрузка данных](#Загрузка-данных)\n",
    "4. [Тренировка и хостинг модели](#Тренировка-и-хостинг-модели)\n",
    "5. [Инференс](#Инференс)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "Этот ноутбук покажет как классифицировать рукописные цифры из датасета MNIST с использованием алгоритма XGboost на Amazon Sagemaker с использованием библиотеки SageMaker PySpark. После тренировки мы будем хостить модель на Amazon Sagemaker и получать предсказания с помощью нее.\n",
    "\n",
    "Больше информации о Sagemaker Spark можно найти по ссылке на GitHub: https://github.com/aws/sagemaker-spark\n",
    "\n",
    "Также можно посетить репозиторий https://github.com/dmlc/xgboost для того, чтобы узнать больше о XGboost алгоритме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка\n",
    "\n",
    "Для начала, мы импортируем необходимые библиотеки и создаем SparkSession() с использованием зависимостей SageMaker Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker_pyspark\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# Конфигурация для использования SageMaker Spark зависимостей\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "# sagemaker-pyspark-sdk секция репозитория SageMaker Spark\n",
    "# покажет как коннектиться к удаленному EMR кластеру \n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\\\n",
    "    .master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных\n",
    "\n",
    "libsvm формат MNIST датасета, который мы будем использовать для тренировки нашей модели доступен в s3-bucket:\n",
    "\n",
    "`s3://sagemaker-sample-data-[region]/spark/mnist/train/`\n",
    "\n",
    "где [region] меняется на регион aws, в котором развернуты и работают ваши сервисы.\n",
    "\n",
    "Больше информации о формате libsvm, использующемся в данном ноутбуке, можно найти по адресу: https://spark.apache.org/docs/2.0.2/mllib-data-types.html\n",
    "\n",
    "Информация о коннекте SageMaker Notebook Instance к удаленному EMR кластеру можно найти по ссылке: https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cn_regions = ['cn-north-1', 'cn-northwest-1']\n",
    "region = boto3.Session().region_name\n",
    "endpoint_domain = 'com.cn' if region in cn_regions else 'com'\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 's3.{}.amazonaws.{}'.format(region, endpoint_domain))\n",
    "\n",
    "trainingData = spark.read.format('libsvm')\\\n",
    "    .option('numFeatures', '784')\\\n",
    "    .option('vectorType', 'dense')\\\n",
    "    .load('s3a://sagemaker-sample-data-{}/spark/mnist/train/'.format(region))\n",
    "\n",
    "testData = spark.read.format('libsvm')\\\n",
    "    .option('numFeatures', '784')\\\n",
    "    .option('vectorType', 'dense')\\\n",
    "    .load('s3a://sagemaker-sample-data-{}/spark/mnist/test/'.format(region))\n",
    "\n",
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренировка и хостинг модели\n",
    "\n",
    "Сейчас мы создадим XGBoostSageMakerEstimator, который использует XGBoost SageMaker алгоритм для тренировки наших входящих данных и использует SageMaker docker image для хостинга нашей модели.\n",
    "\n",
    "Вызывая метод fit(), мы обучим нашу модель на SageMaker и затем создадим endpoint для хостинга. \n",
    "\n",
    "Данный код запускает тренировку и создает endpoint. Данный процесс занимает около 15-20 минут:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sagemaker_pyspark import IAMRole, S3DataPath\n",
    "from sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n",
    "\n",
    "xgboost_estimator = XGBoostSageMakerEstimator(\n",
    "    sagemakerRole=IAMRole(role),\n",
    "    trainingInstanceType='ml.m4.xlarge',\n",
    "    trainingInstanceCount=1,\n",
    "    endpointInstanceType='ml.m4.xlarge',\n",
    "    endpointInitialInstanceCount=1)\n",
    "\n",
    "xgboost_estimator.setEta(0.2)\n",
    "xgboost_estimator.setGamma(4)\n",
    "xgboost_estimator.setMinChildWeight(6)\n",
    "xgboost_estimator.setSilent(0)\n",
    "xgboost_estimator.setObjective(\"multi:softmax\")\n",
    "xgboost_estimator.setNumClasses(10)\n",
    "xgboost_estimator.setNumRound(10)\n",
    "\n",
    "# тренировка модели\n",
    "model = xgboost_estimator.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инференс\n",
    "\n",
    "Теперь мы трансформим наш датафрейм.\n",
    "\n",
    "Для того, чтобы сделать это, мы сериализируем каждую строчку \"фичей\" векторов в libsvm формат для выполнения инференс задач в Sagemaker endpoint. Полученный csv ответ от XGboost мы десериализируем и помещаем в наш датафрейм. Эта сериализация/десериализация происходит в методе transform():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedData = model.transform(testData)\n",
    "\n",
    "transformedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как хорошо отработал алгоритм? Нарисуем цифры, относящиеся к каждому классу и вручную оценим полученные результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# helper функция для отрисовки цифр\n",
    "def show_digit(img, caption='', xlabel='', subplot=None):\n",
    "    if subplot==None:\n",
    "        _,(subplot)=plt.subplots(1,1)\n",
    "    imgr=img.reshape((28,28))\n",
    "    subplot.axes.get_xaxis().set_ticks([])\n",
    "    subplot.axes.get_yaxis().set_ticks([])\n",
    "    plt.title(caption)\n",
    "    plt.xlabel(xlabel)\n",
    "    subplot.imshow(imgr, cmap='gray')\n",
    "\n",
    "images = np.array(transformedData.select(\"features\").cache().take(250))\n",
    "clusters = transformedData.select(\"prediction\").cache().take(250)\n",
    "\n",
    "for cluster in range(10):\n",
    "    print('\\n\\n\\nCluster {}:'.format(int(cluster)))\n",
    "    digits=[ img for l, img in zip(clusters, images) if int(l.prediction) == cluster ]\n",
    "    height=((len(digits) - 1) // 5) + 1\n",
    "    width=5\n",
    "    plt.rcParams[\"figure.figsize\"] = (width,height)\n",
    "    _, subplots = plt.subplots(height, width)\n",
    "    subplots=np.ndarray.flatten(subplots)\n",
    "    for subplot, image in zip(subplots, digits):\n",
    "        show_digit(image, subplot=subplot)\n",
    "    for subplot in subplots[len(digits):]:\n",
    "        subplot.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После успешного выполнения задачи, endpoint может быть удален:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Удаление endpoint\n",
    "\n",
    "from sagemaker_pyspark import SageMakerResourceCleanup\n",
    "\n",
    "resource_cleanup = SageMakerResourceCleanup(model.sagemakerClient)\n",
    "resource_cleanup.deleteResources(model.getCreatedResources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
